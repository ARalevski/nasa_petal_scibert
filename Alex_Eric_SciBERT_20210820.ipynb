{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Alex_Eric_SciBERT_20210820.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RzHMI-2XEGE"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 767
        },
        "id": "zFo0J5V12Kyn",
        "outputId": "74a5eae4-fb1a-40ac-e921-08e55dcc25cf"
      },
      "source": [
        "# !pip install wandb -qqq\n",
        "# import wandb\n",
        "# wandb.login()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.6 MB 5.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 97 kB 6.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 133 kB 43.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 170 kB 52.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.7 MB/s \n",
            "\u001b[?25h  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    618\u001b[0m         \"\"\"\n\u001b[0;32m--> 619\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    620\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0e0c63929346>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install wandb -qqq'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36mlogin\u001b[0;34m(anonymous, key, relogin, host, force)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mconfigured\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_login\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mconfigured\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36m_login\u001b[0;34m(anonymous, key, relogin, host, force, _backend, _silent, _disable_warning)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0mwlogin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;31m# make sure login credentials get to the backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36mprompt_api_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mapi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mno_offline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforce\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mno_create\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforce\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         )\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/lib/apikey.py\u001b[0m in \u001b[0;36mprompt_api_key\u001b[0;34m(settings, api, input_callback, browser_callback, no_offline, no_create, local)\u001b[0m\n\u001b[1;32m    118\u001b[0m                 )\n\u001b[1;32m    119\u001b[0m             )\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_ask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0mwrite_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mgetpass\u001b[0;34m(self, prompt, stream)\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m         )\n\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9STJ25g92UHh"
      },
      "source": [
        "# for run in range(1):\n",
        "  # 1️⃣ Start a new run to track this script\n",
        "  # wandb.init(\n",
        "  #     # Set entity to specify your username or team name\n",
        "  #     # ex: entity=\"aralevski\",\n",
        "  #     # Set the project where this run will be logged\n",
        "  #     project=\"SciBERT\", \n",
        "  #     # Track hyperparameters and run metadata\n",
        "  #     config={\n",
        "  #     \"learning_rate\": 0.02,\n",
        "  #     \"architecture\": \"CNN\",\n",
        "  #     \"dataset\": \"golden_abbr.csv\",})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oizh_miGoSQL",
        "outputId": "4f309c6f-3c67-43a0-9ddc-47716408a6ce"
      },
      "source": [
        "!git clone https://github.com/nasa-petal/PeTaL-labeller.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'PeTaL-labeller'...\n",
            "remote: Enumerating objects: 16194, done.\u001b[K\n",
            "remote: Counting objects: 100% (1660/1660), done.\u001b[K\n",
            "remote: Compressing objects: 100% (997/997), done.\u001b[K\n",
            "remote: Total 16194 (delta 896), reused 1363 (delta 635), pack-reused 14534\u001b[K\n",
            "Receiving objects: 100% (16194/16194), 347.16 MiB | 20.62 MiB/s, done.\n",
            "Resolving deltas: 100% (5270/5270), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EViIQ7Tv4s0_",
        "outputId": "4ac70127-7880-4f37-f96a-d8d28d949d9e"
      },
      "source": [
        "%cd PeTaL-labeller/auto-labeler/scibert_new_tokens/\n",
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/PeTaL-labeller/auto-labeler/scibert_new_tokens\n",
            "auto_labeler_prototype_scibert.py\t\t     test.py\n",
            "sagemaker\t\t\t\t\t     training_loss.png\n",
            "SINGLELABEL_Colleen_and_Alex_training_data_4.19.csv  train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUUkgIgl4w-E",
        "outputId": "4e28ed81-9935-4571-f4c8-23f61e077004"
      },
      "source": [
        "!pip install transformers\n",
        "import nltk\n",
        "nltk.download(\"punkt\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.12.3-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 5.3 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 22.6 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.1.2-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 6.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 41.1 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 34.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.1.2 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.3\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jC2EHi9uXIpo"
      },
      "source": [
        "# Dataset Construction\n",
        "\n",
        "Construct `golden.csv` to feed into SciBERT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJsEeMu07UoG"
      },
      "source": [
        "Find a copy of `golden.json`. Here we do this by downloading the PeTaL dataset using `setup.py` in MATCH, but you can also find it in David's `90-golden-json-copy` branch in the `data-collection-and-prep` repository on GitHub."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFXXLSkhXK5i",
        "outputId": "2828d9f5-bf25-43ed-c802-de0d1b2872ae"
      },
      "source": [
        "%cd ../MATCH\n",
        "!python setup.py"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/PeTaL-labeller/auto-labeler/MATCH\n",
            "[2021-11-09 17:23:18,035:urllib3.connectionpool] Starting new HTTPS connection (1): drive.google.com:443\n",
            "[2021-11-09 17:23:18,284:urllib3.connectionpool] https://drive.google.com:443 \"GET /uc?id=1MbmyMzUkd-ke4Tnl-scTuspPmnWcRb6x HTTP/1.1\" 200 None\n",
            "[2021-11-09 17:23:18,289:urllib3.connectionpool] Starting new HTTPS connection (1): docs.google.com:443\n",
            "[2021-11-09 17:23:18,486:urllib3.connectionpool] https://docs.google.com:443 \"GET /uc?export=download&confirm=dake&id=1MbmyMzUkd-ke4Tnl-scTuspPmnWcRb6x HTTP/1.1\" 200 None\n",
            "[2021-11-09 17:23:18,661:urllib3.connectionpool] https://docs.google.com:443 \"GET /uc?export=download&confirm=Avvm&id=1MbmyMzUkd-ke4Tnl-scTuspPmnWcRb6x HTTP/1.1\" 302 None\n",
            "[2021-11-09 17:23:18,664:urllib3.connectionpool] Starting new HTTPS connection (1): doc-00-20-docs.googleusercontent.com:443\n",
            "[2021-11-09 17:23:18,693:urllib3.connectionpool] https://doc-00-20-docs.googleusercontent.com:443 \"GET /docs/securesc/51vncomb5mgbilc86ucbn10en2rklkk1/8sqip63tt6lorloghknkhamof1fmbc3h/1636478550000/07341482379174037459/08922928825149207716Z/1MbmyMzUkd-ke4Tnl-scTuspPmnWcRb6x?e=download HTTP/1.1\" 302 0\n",
            "[2021-11-09 17:23:18,719:urllib3.connectionpool] https://docs.google.com:443 \"GET /nonceSigner?nonce=mdejs74gru5jq&continue=https://doc-00-20-docs.googleusercontent.com/docs/securesc/51vncomb5mgbilc86ucbn10en2rklkk1/8sqip63tt6lorloghknkhamof1fmbc3h/1636478550000/07341482379174037459/08922928825149207716Z/1MbmyMzUkd-ke4Tnl-scTuspPmnWcRb6x?e%3Ddownload&hash=cv9cri7cq2ab90n98p6opsbuuq2giiq8 HTTP/1.1\" 302 0\n",
            "[2021-11-09 17:23:19,063:urllib3.connectionpool] https://doc-00-20-docs.googleusercontent.com:443 \"GET /docs/securesc/51vncomb5mgbilc86ucbn10en2rklkk1/8sqip63tt6lorloghknkhamof1fmbc3h/1636478550000/07341482379174037459/08922928825149207716Z/1MbmyMzUkd-ke4Tnl-scTuspPmnWcRb6x?e=download&nonce=mdejs74gru5jq&user=08922928825149207716Z&hash=4k28c58bjc6hd910hm9mrmg7agc1qjsr HTTP/1.1\" 200 68771840\n",
            "PeTaL/filtered.json\n",
            "PeTaL/golden.json\n",
            "PeTaL/PeTaL.joint.emb\n",
            "PeTaL/taxonomy.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsIM3kbL8phQ"
      },
      "source": [
        "Open `golden.json`, convert it to csv so it follows the same format as `SINGLELABEL_Colleen_and_Alex_training_data_4.19.csv`, and write out to `golden.csv`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWOoPaRAYSlC"
      },
      "source": [
        "import json\n",
        "\n",
        "with open('src/MATCH/PeTaL/golden.json') as fin:\n",
        "    golden = json.loads(fin.read())"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kprbYa4Ym85"
      },
      "source": [
        "def convert_json_to_csv(js):\n",
        "    title = ' '.join(js['title'])\n",
        "    abstract = ' '.join(js['abstract'])\n",
        "    single_labels = js['isBiomimicry']\n",
        "    return ',\"' + title + '\",\"' + abstract + '\",,,,' + single_labels + \",\""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8c4Fibeannb"
      },
      "source": [
        "with open('../scibert_new_tokens/golden.csv', 'w') as fout:\n",
        "    fout.write(',title,abstract,labels,doi,url,single_labels,labels_string\\n')\n",
        "    for js in golden:\n",
        "        fout.write(convert_json_to_csv(js) + '\\n')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdK7e9GHG5u9"
      },
      "source": [
        "Just some analysis we did so we could get the column names"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mN_VWUubYPx",
        "outputId": "4011208b-243b-4014-eca6-055c2fd76bcb"
      },
      "source": [
        "!head -n 2 ../scibert_new_tokens/golden.csv"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ",title,abstract,labels,doi,url,single_labels,labels_string\n",
            ",\"building home foam tungara frog foam nest architecture three phase construction process\",\"frogs build foam nests floating water face problems over dispersion secretions eggs dangerously exposed air interface nest construction behaviour tungara engystomops pustulosus features may circumvent pairs periodic bursts production egg deposition three discrete phases discernible first characterized bubble raft without approximately linear increase duration mixing events time phase reduce initial precursor materials until critical concentration achieved main building marked start intervals nearly constant final change exponential fashion joining colonial nesting abbreviate presumably exploiting pioneer pair thereby reducing energy material expenditure predators finally deposited centre continuously 1 cm deep free cortex protectively encloses hatched larvae stranded\",,,,Y,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZkPoETnc2p1",
        "outputId": "ada1d6b9-63e7-4a24-bdc7-53e5f10a99df"
      },
      "source": [
        "%cd ../scibert_new_tokens"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/PeTaL-labeller/auto-labeler/scibert_new_tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2fvWFBxG-jc"
      },
      "source": [
        "The full `golden.csv` was too big so we used the first 3000 papers instead"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNc3QCvkd38N"
      },
      "source": [
        "!head -n 3001 golden.csv > golden_abbr.csv"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYYZR4jTeExB",
        "outputId": "523b3b59-b31a-4aa0-e315-226f78ff6e59"
      },
      "source": [
        "!wc golden_abbr.csv"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   3001  344028 2895522 golden_abbr.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIIAl_GqXLVU"
      },
      "source": [
        "# Model Running"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDPNjMJpH5N4"
      },
      "source": [
        "Okay, first, here's what `auto_labeler_prototype_scibert.py` contains for the function `setup_dataset()` (I think the rest is unchanged):\n",
        "\n",
        "```\n",
        "def setup_dataset():\n",
        "    \"\"\"Setup the Train and Validation datasets\n",
        "\n",
        "    Returns:\n",
        "        (tuple): tuple containing:\n",
        "\n",
        "            - **train_dataset** (torch.utils.data.TensorDataset): pitch distribution\n",
        "            - **val_dataset** (torch.utils.data.TensorDataset): pitch to chord distribution\n",
        "    \"\"\"\n",
        "    ########################################\n",
        "    # WE CHANGED filename IN OUR MEETING\n",
        "    ########################################\n",
        "    filename = 'golden_abbr.csv' # 'golden.csv' # 'SINGLELABEL_Colleen_and_Alex_training_data_4.19.csv'\n",
        "    if (not os.path.exists(filename)):\n",
        "        url = 'https://drive.google.com/file/d/1eOLNOl6ZMz4UxQ7qbSI-bJSSNkmNZjr9/view?usp=sharing'\n",
        "        gdown.download(url, filename, quiet=False)\n",
        "        md5 = 'fa837a88f0c40c513d975104edf3da17'\n",
        "        gdown.cached_download(url, filename, md5=md5, postprocess=gdown.extractall)\n",
        "\n",
        "    df = pd.read_csv(filename)\n",
        "    df = df[['title', 'abstract', 'labels', 'doi', 'url', 'single_labels', 'labels_string']]\n",
        "    df = df[df['single_labels'].notnull()]\n",
        "\n",
        "    labels = []\n",
        "    docs = []\n",
        "    labels_test = []\n",
        "    docs_test = []\n",
        "    ########################################\n",
        "    # WE ALSO CHANGED labels_dict AND I THINK THAT'S IT?\n",
        "    ########################################\n",
        "    labels_dict = ['Y', 'N'] # [\"'Protect from harm'\", \"'Process resources'\", \"'Sense send or process information'\", \"'Maintain structural integrity'\", \"'Move'\", \"'Attach'\", \"'Maintain ecological community'\", \"'Chemically modify or Change energy state'\", \"'Change size or color'\", \"'Physically assemble/disassemble'\"]\n",
        "\n",
        "    single_labels = df[\"single_labels\"].tolist()\n",
        "    abstract = df[\"abstract\"].tolist()\n",
        "    title = df[\"title\"].tolist()\n",
        "    for i in range(len(title)):\n",
        "        if i < len(title) - 40:\n",
        "            docs.append(abstract[i])\n",
        "            labels.append(labels_dict.index(single_labels[i]))\n",
        "        else:\n",
        "            docs_test.append(abstract[i])\n",
        "            labels_test.append(labels_dict.index(single_labels[i]))\n",
        "\n",
        "    print(\"Number of training labels: {:}\".format(len(labels)))\n",
        "    print(\"Number of training docs: {:}\".format(len(docs)))\n",
        "    print(\"Number of test labels: {:}\".format(len(labels_test)))\n",
        "    print(\"Number of test docs: {:}\".format(len(docs_test)))\n",
        "    \n",
        "    regular_list = [word_tokenize(a) for a in abstract]\n",
        "    tokens_flat = [item for sublist in regular_list for item in sublist]\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
        "    tokenizer.add_tokens(tokens_flat)\n",
        "\n",
        "    print('SciBERT tokenizer loaded')\n",
        "\n",
        "    # original abstract\n",
        "    print(' Original: ', docs[5])\n",
        "    # abstract split into tokens\n",
        "    print('Tokenized: ', tokenizer.tokenize(docs[5]))\n",
        "    # abstract as mapped to ids\n",
        "    print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(docs[5])))\n",
        "   \n",
        "    # Finishing tokenizing all docs and map tokens to thier word IDs\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    actual_labels_test = []\n",
        "\n",
        "    for d in docs:\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "                            d,                      \n",
        "                            truncation=True,\n",
        "                            add_special_tokens = True, \n",
        "                            max_length = 256,           \n",
        "                            pad_to_max_length = True,\n",
        "                            return_attention_mask = True,   \n",
        "                            return_tensors = 'pt',     \n",
        "                    )\n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "    # Convert the lists into tensors.\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "    labels = torch.tensor(labels)\n",
        "\n",
        "    print('Original: ', docs[5])\n",
        "    print('Token IDs:', input_ids[5])\n",
        "    print('Reverse:', tokenizer.convert_ids_to_tokens(input_ids[5]))\n",
        "\n",
        "    # Split up training & testing/validation\n",
        "    dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "    # Number of docs to include per set\n",
        "    train_size = int(0.9 * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "    return train_dataset, val_dataset, tokenizer\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I29okNWPHFbz"
      },
      "source": [
        "Now you run it! This part took an hour and 53 seconds to run for me on Colab\n",
        "\n",
        "Also note that you may want to change the number of test labels/docs (it's hard-coded in `auto_labeler_prototype_scibert.py`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZ-IYl0F44Y4",
        "outputId": "c6d628f7-a2f9-4b76-ad9f-fec0595540c5"
      },
      "source": [
        "!python binary_labeler_scibert.py --epochs=10\n",
        "# wandb.log({\"acc\":acc, \"loss\":loss})\n",
        "\n",
        "  # Mark the run as finished\n",
        "# wandb.finish()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file 'binary_labeler_scibert.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    }
  ]
}